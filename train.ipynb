{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.policies import BaseFeaturesExtractor\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from kaggle_environments import make, evaluate\n",
    "\n",
    "from tool.check_win import check_win, get_position\n",
    "from tool.check_three import check_three\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# env.step\n",
    "回傳值:(obs, reward, done , _)\n",
    "## obs\n",
    "- 只有board有用?\n",
    "## reward\n",
    "- 贏了是1\n",
    "- 平常是0\n",
    "## done\n",
    "- 遊戲是否結束"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建置openAI GYM的環境\n",
    "#code from: https://www.kaggle.com/code/alexisbcook/deep-reinforcement-learning\n",
    "class ConnectFourGym(gym.Env):\n",
    "    def __init__(self, agent2=\"random\"):\n",
    "        ks_env = make(\"connectx\", debug=True)\n",
    "        self.env = ks_env.train([None, agent2])\n",
    "        self.rows = ks_env.configuration.rows\n",
    "        self.columns = ks_env.configuration.columns\n",
    "        # Learn about spaces here: http://gym.openai.com/docs/#spaces\n",
    "        self.action_space = spaces.Discrete(self.columns)\n",
    "        self.observation_space = spaces.Box(low=0, high=2, \n",
    "                                            shape=(1,self.rows,self.columns), dtype=int)\n",
    "        # Tuple corresponding to the min and max possible rewards\n",
    "        self.reward_range = (-10, 10)\n",
    "        # StableBaselines throws error if these are not defined\n",
    "        self.spec = None\n",
    "        self.metadata = None\n",
    "    def reset(self):\n",
    "        self.obs = self.env.reset()\n",
    "        return np.array(self.obs['board']).reshape(1,self.rows,self.columns)\n",
    "    def change_reward(self, action : int, old_reward : int, done : bool):\n",
    "\n",
    "        #決勝\n",
    "        if (done):\n",
    "            if (old_reward == 1): return 10\n",
    "            else: return -10\n",
    "        \n",
    "\n",
    "        board = np.array(self.obs['board'])\n",
    "        board = list(board.reshape(-1))\n",
    "        \n",
    "        #對面差一顆就能連成4的數量\n",
    "        for i in range(7):\n",
    "            if (check_win(board, self.rows, self.columns, i, 2) and i == action): \n",
    "                return 1\n",
    "        \n",
    "        #差一顆就能連成4的數量\n",
    "        posible_three = check_three(board, self.rows, self.columns, action, 1)\n",
    "        \n",
    "\n",
    "        if (posible_three >= 2): return posible_three * 0.5\n",
    "\n",
    "\n",
    "        return 0\n",
    "            \n",
    "        \n",
    "    def step(self, action):\n",
    "        # Check if agent's move is valid\n",
    "        is_valid = (self.obs['board'][int(action)] == 0)\n",
    "        if is_valid: # Play the move\n",
    "            new_obs, old_reward, done, _ = self.env.step(int(action))\n",
    "            reward = self.change_reward(action, old_reward, done)\n",
    "            self.obs = new_obs\n",
    "\n",
    "\n",
    "        else: # End the game and penalize agent\n",
    "            reward, done, _ = -20, True, {}\n",
    "        return np.array(self.obs['board']).reshape(1,self.rows,self.columns), reward, done, _\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#讓agent更好的抓取圖像特徵\n",
    "class CustomCNNFeatureExtractor(BaseFeaturesExtractor): # Custom\n",
    "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 256):\n",
    "        super(CustomCNNFeatureExtractor, self).__init__(observation_space, features_dim)\n",
    "        \n",
    "        input_channel = observation_space.shape[0]\n",
    "        self.output_shape = observation_space.shape[1]\n",
    "\n",
    "        \n",
    "        self.input_cnn = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_channel, out_channels=32, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=2, stride=1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Flatten())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            n_flatten = self.input_cnn(torch.as_tensor(observation_space.sample()[None]).float()).shape[1]\n",
    "            \n",
    "\n",
    "        \n",
    "        self.output = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.LeakyReLU())\n",
    "\n",
    "\n",
    "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.input_cnn(observations)\n",
    "        \n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def opponent(obs, config):\n",
    "    valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]\n",
    "    for i in valid_moves:\n",
    "        if (check_win(board=obs.board, row=config.rows, col=config.columns, choice=i, player=2)):\n",
    "            return i\n",
    "    for i in valid_moves:\n",
    "        if (check_win(board=obs.board, row=config.rows, col=config.columns, choice=i, player=1)):\n",
    "            return i\n",
    "            \n",
    "\n",
    "    return random.choice(valid_moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomCNNFeatureExtractor,\n",
    "    )\n",
    "env = ConnectFourGym(agent2=opponent)\n",
    "\n",
    "model = PPO(\"CnnPolicy\", env, policy_kwargs=policy_kwargs, learning_rate=0.0001, n_steps=4096, n_epochs=20, verbose=1, batch_size=256)\n",
    "\n",
    "# model.policy.mlp_extractor.policy_net = nn.Sequential(nn.Linear(256, 128),\n",
    "#                                                     nn.LeakyReLU(),\n",
    "#                                                     nn.Linear(128, 64,),\n",
    "#                                                     nn.Tanh())\n",
    "# model.policy.mlp_extractor.value_net = nn.Sequential(nn.Linear(256, 128),\n",
    "#                                                     nn.LeakyReLU(),\n",
    "#                                                     nn.Linear(128, 64,),\n",
    "#                                                     nn.Tanh())\n",
    "# model.policy.mlp_extractor.policy_net = \\\n",
    "#     model.policy.mlp_extractor.policy_net.to(device=\"cuda\", dtype=torch.float32)\n",
    "# model.policy.mlp_extractor.value_net = \\\n",
    "#     model.policy.mlp_extractor.value_net.to(device=\"cuda\", dtype=torch.float32)\n",
    "\n",
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=204800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = model.policy\n",
    "win_p = 0\n",
    "def use_policy(obs, config):\n",
    "    \n",
    "\n",
    "    #將資料整理成輸入格式\n",
    "    board = np.array(obs[\"board\"])\n",
    "    board = board.reshape(1, 1, config.rows, config.columns)\n",
    "    board_tensor = torch.tensor(board)\n",
    "    \n",
    "\n",
    "    board_tensor = board_tensor.to(device=\"cuda\", dtype=torch.float32)\n",
    "\n",
    "\n",
    "    predict = int(policy.forward(board_tensor)[0])\n",
    "\n",
    "\n",
    "    # valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]\n",
    "    # for i in valid_moves:\n",
    "    #     if (check_win(board=obs.board, row=config.rows, col=config.columns, choice=i, player=2)):\n",
    "    #         predict = i\n",
    "    # for i in valid_moves:\n",
    "    #     if (check_win(board=obs.board, row=config.rows, col=config.columns, choice=i, player=1)):\n",
    "    #         predict = i\n",
    "\n",
    "    # #陽春顯示畫面....\n",
    "    # new_board, _ = get_position(board.reshape(config.rows, config.columns), predict, 1)\n",
    "    # print(new_board)\n",
    "    # global win_p\n",
    "    \n",
    "    # win = check_win(list(board), config.rows, config.columns, predict, 1)\n",
    "    # if (win): win_p += 1\n",
    "    #print(win)\n",
    "    return predict\n",
    "\n",
    "def human_play(obs, config):\n",
    "    board = np.array(obs[\"board\"])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_win_percentages(agent1, agent2, n_rounds=100):\n",
    "    # Use default Connect Four setup\n",
    "    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n",
    "    # Agent 1 goes first (roughly) half the time          \n",
    "    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n",
    "    # Agent 2 goes first (roughly) half the time      \n",
    "    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n",
    "    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])/len(outcomes), 2))\n",
    "    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])/len(outcomes), 2))\n",
    "    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n",
    "    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_win_percentages(use_policy, opponent, 500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(\"connect_x_tie_048\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
