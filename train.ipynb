{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "termcolor not installed, skipping dependency\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.policies import BaseFeaturesExtractor\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from kaggle_environments import make, evaluate\n",
    "\n",
    "from tool.check_win import check_win, get_position\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'remainingOverageTime': 60, 'step': 2, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0], 'mark': 1}, 0, False, {}]\n",
      "[{'remainingOverageTime': 60, 'step': 4, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0], 'mark': 1}, 0, False, {}]\n",
      "[{'remainingOverageTime': 60, 'step': 6, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 1, 0, 2, 0, 0, 0], 'mark': 1}, 0, False, {}]\n",
      "[{'remainingOverageTime': 60, 'step': 8, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 1, 0, 2, 0, 2, 0], 'mark': 1}, 0, False, {}]\n"
     ]
    }
   ],
   "source": [
    "ks_env = make(\"connectx\", debug=True)\n",
    "env = ks_env.train([None, \"random\"])\n",
    "print(env.step(1))\n",
    "print(env.step(1))\n",
    "print(env.step(1))\n",
    "print(env.step(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# env.step\n",
    "回傳值:(obs, reward, done , _)\n",
    "## obs\n",
    "- 只有board有用?\n",
    "## reward\n",
    "- 贏了是1\n",
    "- 平常是0\n",
    "## done\n",
    "- 遊戲是否結束"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建置openAI GYM的環境\n",
    "#code from: https://www.kaggle.com/code/alexisbcook/deep-reinforcement-learning\n",
    "class ConnectFourGym(gym.Env):\n",
    "    def __init__(self, agent2=\"random\"):\n",
    "        ks_env = make(\"connectx\", debug=True)\n",
    "        self.env = ks_env.train([None, agent2])\n",
    "        self.rows = ks_env.configuration.rows\n",
    "        self.columns = ks_env.configuration.columns\n",
    "        # Learn about spaces here: http://gym.openai.com/docs/#spaces\n",
    "        self.action_space = spaces.Discrete(self.columns)\n",
    "        self.observation_space = spaces.Box(low=0, high=2, \n",
    "                                            shape=(1,self.rows,self.columns), dtype=int)\n",
    "        # Tuple corresponding to the min and max possible rewards\n",
    "        self.reward_range = (-10, 10)\n",
    "        # StableBaselines throws error if these are not defined\n",
    "        self.spec = None\n",
    "        self.metadata = None\n",
    "    def reset(self):\n",
    "        self.obs = self.env.reset()\n",
    "        return np.array(self.obs['board']).reshape(1,self.rows,self.columns)\n",
    "    def change_reward(self, action, old_reward, done):\n",
    "        if (done):\n",
    "            if (old_reward == 1): return 10\n",
    "            else: return -5\n",
    "        \n",
    "\n",
    "        board = np.array(self.obs['board'])\n",
    "        board = list(board.reshape(-1))\n",
    "        \n",
    "        #check try to prevent lost\n",
    "        for i in range(7):\n",
    "            if (check_win(board, self.rows, self.columns, action, 2) and i == action): return 5\n",
    "        \n",
    "        #check three\n",
    "            \n",
    "\n",
    "\n",
    "        return 1 / 42\n",
    "            \n",
    "        \n",
    "    def step(self, action):\n",
    "        # Check if agent's move is valid\n",
    "        is_valid = (self.obs['board'][int(action)] == 0)\n",
    "        if is_valid: # Play the move\n",
    "            new_obs, old_reward, done, _ = self.env.step(int(action))\n",
    "            reward = self.change_reward(action, old_reward, done)\n",
    "            self.obs = new_obs\n",
    "\n",
    "\n",
    "        else: # End the game and penalize agent\n",
    "            reward, done, _ = -10, True, {}\n",
    "        return np.array(self.obs['board']).reshape(1,self.rows,self.columns), reward, done, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def opponent(obs, config):\n",
    "    valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]\n",
    "    for i in valid_moves:\n",
    "        if (check_win(board=obs.board, row=config.rows, col=config.columns, choice=i, player=2)):\n",
    "            return i\n",
    "    for i in valid_moves:\n",
    "        if (check_win(board=obs.board, row=config.rows, col=config.columns, choice=i, player=1)):\n",
    "            return i\n",
    "            \n",
    "\n",
    "    return random.choice(valid_moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#讓agent更好的抓取圖像特徵\n",
    "class connect_x_policy(BaseFeaturesExtractor): # Custom\n",
    "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 256):\n",
    "        super(connect_x_policy, self).__init__(observation_space, features_dim)\n",
    "        \n",
    "        input_channel = observation_space.shape[0]\n",
    "        self.output_shape = observation_space.shape[1]\n",
    "\n",
    "        \n",
    "        self.input_cnn = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_channel, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=2, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            n_flatten = self.input_cnn(torch.as_tensor(observation_space.sample()[None]).float()).shape[1]\n",
    "            \n",
    "\n",
    "        \n",
    "        self.output = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
    "\n",
    "\n",
    "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.input_cnn(observations)\n",
    "        \n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x2101383d4c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=connect_x_policy)\n",
    "env = ConnectFourGym(agent2=opponent)\n",
    "model = PPO(\"CnnPolicy\", env, policy_kwargs=policy_kwargs, learning_rate=0.0003, n_steps=2048, n_epochs=10)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = model.policy\n",
    "\n",
    "def use_policy(obs, config):\n",
    "\n",
    "    #將資料整理成輸入格式\n",
    "    board = np.array(obs[\"board\"])\n",
    "    board = board.reshape(1, 1, config.rows, config.columns)\n",
    "    board_tensor = torch.tensor(board)\n",
    "    \n",
    "\n",
    "    board_tensor = board_tensor.to(device=\"cuda\", dtype=torch.float32)\n",
    "\n",
    "\n",
    "    predict = int(policy.forward(board_tensor)[0])\n",
    "\n",
    "\n",
    "    #防止選到不能放棋子的格子\n",
    "    while(board[0, 0, 0, predict] != 0):\n",
    "        predict = int(policy.forward(board_tensor)[0])\n",
    "\n",
    "    #陽春顯示畫面....\n",
    "    new_board, _ = get_position(board.reshape(config.rows, config.columns), predict, 1)\n",
    "    print(new_board)\n",
    "    \n",
    "    win = check_win(list(board), config.rows, config.columns, predict, 1)\n",
    "    print(win)\n",
    "    return predict\n",
    "\n",
    "def human_play(obs, config):\n",
    "    board = np.array(obs[\"board\"])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_win_percentages(agent1, agent2, n_rounds=100):\n",
    "    # Use default Connect Four setup\n",
    "    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n",
    "    # Agent 1 goes first (roughly) half the time          \n",
    "    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n",
    "    # Agent 2 goes first (roughly) half the time      \n",
    "    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n",
    "    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])/len(outcomes), 2))\n",
    "    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])/len(outcomes), 2))\n",
    "    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n",
    "    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1 Win Percentage: 0.74\n",
      "Agent 2 Win Percentage: 0.26\n",
      "Number of Invalid Plays by Agent 1: 0\n",
      "Number of Invalid Plays by Agent 2: 0\n"
     ]
    }
   ],
   "source": [
    "get_win_percentages(use_policy, opponent, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
